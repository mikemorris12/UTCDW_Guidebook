{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87887a0",
   "metadata": {},
   "source": [
    "# 6.3 Edmonton AC Loads: Multi-Scenario Analysis\n",
    "\n",
    "This notebook continues the worked example on analyzing projections of air conditioning loads in Edmonton, using Cooling Degree Days as the climate indicator and weather station data as the observational product. Here we expand the scope of the analysis to include projections not only from multiple CMIP6 models, but also for multiple future scenarios. Here is an updated flowchart, generated using the UTCDW survey:\n",
    "\n",
    "```{image} ./figures/survey_station_multiscenario.png\n",
    ":width: 600px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "As always, we'll begin by importing the necessary packages and acquiring the observational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c528298",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from xclim import sdba\n",
    "from xclim.core.calendar import convert_calendar\n",
    "import xclim.indices as xci\n",
    "import xclim.ensembles as xce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ec3\n",
    "import gcsfs\n",
    "import zarr\n",
    "import os\n",
    "\n",
    "# lat and lon coordinates for Edmonton\n",
    "lat_edm = 53.5\n",
    "lon_edm = -113.5\n",
    "\n",
    "# time periods for historical and future periods\n",
    "years_hist = range(1980, 2011) # remember that range(start, end) is not inclusive of `end`\n",
    "years_future = range(2070, 2101)\n",
    "\n",
    "# url for the CSV file that contains the data catalog\n",
    "url_gcsfs_catalog = 'https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ece2d13",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# get the same station data as from 6.1 and 6.2\n",
    "\n",
    "def download_station_data(stn_id, years = range(1992, 2022)):\n",
    "    \"\"\"\n",
    "    Download ECCC observational data at the weather station identified with stn_id,\n",
    "    re-name the columns to more useful names. Optional: select specific years of data,\n",
    "    but by default, download all data from 1990--2020.\n",
    "    \"\"\"\n",
    "    # download the data\n",
    "    df = ec3.get_data(stn_id, years = years, progress = False)\n",
    "    \n",
    "    # dictionary mapping original column names to new ones\n",
    "    column_name_dict = {'Date/Time': 'time', \n",
    "                        'Mean Temp (Â°C)': 'tas',\n",
    "                        'Latitude (y)': 'lat',\n",
    "                        'Longitude (x)': 'lon',\n",
    "                        'Station Name': 'Name'}\n",
    "    \n",
    "    df = df.rename(columns = column_name_dict)\n",
    "    \n",
    "    # select only the variables we re-named - you can comment this out if you want to keep all variables\n",
    "    df = df[list(column_name_dict.values())]\n",
    "    \n",
    "    # set the \"time\" column as an index column and convert it from strings to Datetime objects to make\n",
    "    # selecting times easier\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df = df.set_index(\"time\")\n",
    "    \n",
    "    # sort the data in proper chronological order\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f093ba",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# download the station data\n",
    "stn_id_list = [1867, 31427]\n",
    "df_list = []\n",
    "for stn_id in stn_id_list:\n",
    "    df = download_station_data(stn_id, years = years_hist)\n",
    "    df_list.append(df)\n",
    "    \n",
    "stn_df = pd.concat(df_list, axis = 0)\n",
    "\n",
    "# now convert it to xarray format for easier use with the model data and xclim\n",
    "\n",
    "# drop lat and lon variables, since we want these to be coordinates in the xr.Dataset\n",
    "stn_lon = stn_df.lon.values[0] + 360 # convert lon to same convention as model data\n",
    "stn_lat = stn_df.lat.values[0] \n",
    "\n",
    "stn_df = stn_df.drop(['lat', 'lon'], axis = 1)\n",
    "\n",
    "stn_ds = xr.Dataset.from_dataframe(stn_df)\n",
    "stn_ds = stn_ds.assign_coords(lat = stn_lat, lon = stn_lon)\n",
    "\n",
    "stn_ds_noleap = convert_calendar(stn_ds, 'noleap')\n",
    "tas_obs_noleap = stn_ds_noleap.tas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee7820",
   "metadata": {},
   "source": [
    "## 6.3.1 Creating a Multi-Model, Multi-Scenario Ensemble\n",
    "\n",
    "Now we'll search the catalog for data from the same models as in 6.2, but for multiple SSP scenarios. We'll use the 4 most common SSPs, listed in increasing order of end-of-century radiative forcing:\n",
    "* SSP1-2.6 (Sustainability)\n",
    "* SSP2-4.5 (Middle of the Road)\n",
    "* SSP3-7.0 (Regionaly Rivalry)\n",
    "* SSP5-8.5 (Fossil-Fueled Development)\n",
    "\n",
    "For each model, we'll again use only a single ensemble member. The code below will search the Google Cloud CMIP6 data catalog for ensemble members that include the historical simulation, plus branches for each of the 4 SSPs listed above. For consistency between the historical data and future projections, they must all come from the same ensemble member. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990da6b",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# open the Google Cloud model data catalog with pandas\n",
    "df_catalog = pd.read_csv(url_gcsfs_catalog)\n",
    "\n",
    "# search for entries which have daily tas data from the selected scenarios and models\n",
    "scenarios = ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "\n",
    "models = ['AWI-CM-1-1-MR', 'CESM2', 'CanESM5', 'EC-Earth3', \n",
    "          'FGOALS-g3','INM-CM5-0', 'IPSL-CM6A-LR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0']\n",
    "\n",
    "search_string_mm = \"table_id == 'day' & variable_id == 'tas'\"\n",
    "search_string_mm += f\" & experiment_id == {scenarios}\"\n",
    "search_string_mm += f\"& source_id == {models}\"\n",
    "df_search_mm = df_catalog.query(search_string_mm)\n",
    "\n",
    "# for each model, select only one ensemble member\n",
    "df_search_mm = df_search_mm.sort_values('member_id')\n",
    "df_search_mm_onemember = df_search_mm.drop_duplicates(['source_id', 'experiment_id'], keep = 'first')\n",
    "df_search_mm_onemember = df_search_mm_onemember.sort_values('source_id')\n",
    "df_search_mm_onemember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c408e51",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ideally we'd use only one historical ensemble member for each model, and have all the SSP scenarios\n",
    "# for that model branch from the same historical simulation. This has worked out for most of the models,\n",
    "# but not IPSL or INM. Let's see if we can find a single member which works for those models\n",
    "\n",
    "for model in ['IPSL-CM6A-LR', 'INM-CM5-0']:\n",
    "    print(model)\n",
    "    # drop the original results for this model from the search results\n",
    "    where_current_model = df_search_mm_onemember[df_search_mm_onemember.source_id == model].index\n",
    "    df_search_mm_onemember = df_search_mm_onemember.drop(where_current_model)\n",
    "    \n",
    "    search_model = f\"source_id == '{model}' & table_id == 'day' & variable_id == 'tas' & experiment_id == {scenarios}\"\n",
    "    df_search_model = df_catalog.query(search_model)\n",
    "    df_search_model = df_search_model.sort_values('member_id')\n",
    "    exps_per_member = df_search_model.groupby('member_id').size()\n",
    "    \n",
    "    # search for ensemble members that have the same number of entries as there are scenarios in our list\n",
    "    candidate_members = exps_per_member.loc[exps_per_member == len(scenarios)].index.sort_values()\n",
    "\n",
    "    # take the first one, if there are any results, and append its entries to the search results\n",
    "    if len(candidate_members) > 0:\n",
    "        member = candidate_members[0]\n",
    "        results_for_this_model = df_search_model[df_search_model.member_id == member]\n",
    "        df_search_mm_onemember = pd.concat([df_search_mm_onemember, results_for_this_model])\n",
    "        \n",
    "df_search_mm_onemember[df_search_mm_onemember.source_id.isin(['IPSL-CM6A-LR', 'INM-CM5-0'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf25b16",
   "metadata": {},
   "source": [
    "Having found entries that suit our needs, let's download the data and format it using `xclim.ensembles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c26ccd",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# function for pre-processing data\n",
    "def get_and_process_data(catalog_df, model, scenario, gcs, lat, lon, years):\n",
    "    # get the ztore url for this model and scenario\n",
    "    df_scen = catalog_df.query(f\"source_id == '{model}' & experiment_id == '{scenario}'\")\n",
    "    zstore_url = df_scen.zstore.values[0]\n",
    "    \n",
    "    # get the GCS mapper from the url\n",
    "    mapper = gcs.get_mapper(zstore_url)\n",
    "    \n",
    "    # open the file with xarray\n",
    "    ds = xr.open_zarr(mapper, consolidated = True)\n",
    "    \n",
    "    # get the tas data, select the time period, and interp to the desired location\n",
    "    tas_loc = ds.tas.sel(time = ds.time.dt.year.isin(years)).interp(lat = lat, lon = lon)\n",
    "    \n",
    "    # drop 'height' coordinate, which is always 2m but isn't present on all datasets\n",
    "    if 'height' in tas_loc.coords.keys():\n",
    "        tas_loc = tas_loc.reset_coords('height', drop = True)\n",
    "        \n",
    "    # some datasets put the date at 12:00 whereas some put it at 00:00. To make all\n",
    "    # of them consistent, simply change the time coordinate to the date only\n",
    "    tas_loc = tas_loc.assign_coords(time = tas_loc.time.dt.floor('D'))\n",
    "    # convert from Kelvin to Celsius and return\n",
    "    \n",
    "    tas_loc = tas_loc - 273.15\n",
    "    return tas_loc.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_multimodel_multiscen(catalog_df, gcs, models, scenarios,\n",
    "                                       stn_lat, stn_lon,\n",
    "                                       years_hist, years_future):\n",
    "\n",
    "    ds_list_hist = []\n",
    "    ds_list_future = []\n",
    "    for model in models:\n",
    "        print(f\"========================{model}=============================\")\n",
    "        print('historical')\n",
    "        tas_model_hist = get_and_process_data(catalog_df, model, 'historical', \n",
    "                                              gcs, stn_lat, stn_lon, years_hist)\n",
    "        ds_list_hist.append(tas_model_hist)\n",
    "    \n",
    "        # get the future simulation data for this model, for each scenario\n",
    "        ds_list_scen = []\n",
    "        for scenario in scenarios[1:]: # exclude 'historical' from this iteration\n",
    "            print(scenario)\n",
    "            tas_model_future = get_and_process_data(catalog_df, model, scenario, \n",
    "                                                    gcs, stn_lat, stn_lon, years_future)\n",
    "            ds_list_scen.append(tas_model_future)\n",
    "    \n",
    "        # create ensemble for this one model, where the 'realization' dim represents the different scenarios\n",
    "        ds_future = xce.create_ensemble(ds_list_scen, realizations = scenarios[1:])\n",
    "    \n",
    "        # rename the 'realization' dim\n",
    "        ds_future = ds_future.rename({'realization': 'scenario'})\n",
    "        ds_list_future.append(ds_future)\n",
    "    \n",
    "    print('finished acquiring model data')\n",
    "    \n",
    "    # concatenate the ds_lists together\n",
    "    ds_ens_hist_raw = xce.create_ensemble(ds_list_hist,                             \n",
    "                                          realizations = models,\n",
    "                                          calendar = 'noleap')\n",
    "\n",
    "    ds_ens_future_raw = xce.create_ensemble(ds_list_future, \n",
    "                                          realizations = models,\n",
    "                                          calendar = 'noleap')\n",
    "\n",
    "    # rename 'realization' dimension to 'model'\n",
    "    ds_ens_hist_raw = ds_ens_hist_raw.rename({'realization': 'model'})\n",
    "    ds_ens_future_raw = ds_ens_future_raw.rename({'realization': 'model'})\n",
    "    \n",
    "    # return\n",
    "    return ds_ens_hist_raw, ds_ens_future_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d25a42",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# authenticate access to Google Cloud\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')\n",
    "\n",
    "# file names to save the downloaded data, to save time later when re-running this notebook\n",
    "fout_hist = 'data_files/tas.cmip6.daily.historical.1980-2010.edmonton.nc'\n",
    "fout_future = 'data_files/tas.cmip6.daily.ssp1235.2070-2100.edmonton.nc'\n",
    "\n",
    "# use the function to download the data, this may take a few minutes to run\n",
    "if (not os.path.exists(fout_hist)) or (not os.path.exists(fout_future)):\n",
    "    ds_ens_hist_raw, ds_ens_future_raw = download_data_multimodel_multiscen(df_search_mm_onemember, gcs, models, \n",
    "                                                                            scenarios, stn_lat, stn_lon, \n",
    "                                                                            years_hist, years_future)\n",
    "\n",
    "    # write the data to output files \n",
    "    ds_ens_hist_raw.to_netcdf(fout_hist)\n",
    "    ds_ens_future_raw.to_netcdf(fout_future)\n",
    "else:\n",
    "    # open the files that already exist\n",
    "    ds_ens_hist_raw = xr.open_dataset(fout_hist)\n",
    "    ds_ens_future_raw = xr.open_dataset(fout_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cffe81",
   "metadata": {},
   "source": [
    "## 6.3.2 Exploratory Analysis\n",
    "\n",
    "With this multi-model, multi-scenario ensemble, we can do much of the same analysis as before, and also quantify the effects of scenario uncertainty on the future projections. Since the historical validation for this case would be exactly the same as Section 6.2 (since there's only one historical scenario), we can get right into looking at the raw model future projections. Since we already explored model uncertainty in 6.2, let's look at the raw projections of the multi-model ensemble. Remember though that when we get to the bias-correction, it should be applied to each model individually, **not to the multi-model mean**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the daily climatology, averaged across models, for the historical and future data\n",
    "tas_ens_hist_raw = ds_ens_hist_raw.tas\n",
    "tas_ens_future_raw = ds_ens_future_raw.tas\n",
    "\n",
    "tas_ensmean_hist_raw_clim = tas_ens_hist_raw.groupby('time.dayofyear').mean(('time', 'model')).compute()\n",
    "tas_ensmean_future_raw_clim = tas_ens_future_raw.groupby('time.dayofyear').mean(('time', 'model')).compute()\n",
    "\n",
    "# daily climatology for obs\n",
    "tas_dailyclim_obs = tas_obs_noleap.groupby('time.dayofyear').mean('time').compute()\n",
    "tas_dailyclim_std_obs = tas_obs_noleap.groupby('time.dayofyear').std('time').compute()\n",
    "\n",
    "\n",
    "# add historical and future scenarios to the same DataArray so the xarray plotting routines show it on the legend\n",
    "tas_ensmean_hist_raw_clim = tas_ensmean_hist_raw_clim.assign_coords(scenario = 'historical')\n",
    "tas_clim_model_raw = xr.concat([tas_ensmean_hist_raw_clim, tas_ensmean_future_raw_clim],\n",
    "                                dim = 'scenario')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07696d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# plot the daily climatologies\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "# daily climatologies as 1D curves\n",
    "# obs\n",
    "tas_dailyclim_obs.plot.line(ax = ax, label = \"Station Obs\", color = 'k', linewidth = 3)\n",
    "\n",
    "# models\n",
    "tas_clim_model_raw.plot.line(ax = ax, hue = 'scenario')\n",
    "\n",
    "\n",
    "# 1 sigma shading\n",
    "# obs\n",
    "ax.fill_between(tas_dailyclim_obs.dayofyear,\n",
    "                tas_dailyclim_obs - tas_dailyclim_std_obs, \n",
    "                tas_dailyclim_obs + tas_dailyclim_std_obs,\n",
    "                alpha = 0.2, color = 'k')\n",
    "\n",
    "ax.set_title(\"Edmonton Daily Mean Temperature Daily Climatology - Raw Multi-Model Mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9bcea0",
   "metadata": {},
   "source": [
    "This plot shows how the multi-model average daily mean temperature climatology varies with the future scenario. The historical climatology (blue) shows a slight cold bias relative to the observations (black), which will hopefully be eliminated after bias-correcting the models. The future scenarios all show warming realtive to the historical period, though only the two highest-emission scenarios (SSP3-7.0 and SSP5-8.5) have peaks that exceed the observed range of variability.\n",
    "\n",
    "We should expect that CDDs will increase under each scenario, and the increases will probably be largest for the higher forcing scenarios. However, we already saw that the inter-model spread for projections was quite large for the single-scenario analysis. Will the spread across scenarios be larger? Let's calculate the CDDs before and after bias-adjustment and investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377b097",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# assign unit to temperature data\n",
    "tas_ens_hist_raw.attrs['units'] = 'degC'\n",
    "tas_ens_future_raw.attrs['units'] = 'degC'\n",
    "tas_obs_noleap.attrs['units'] = 'degC'\n",
    "\n",
    "# calculate CDDs\n",
    "cdd_obs =  xci.cooling_degree_days(tas_obs_noleap).compute()\n",
    "cdds_mm_hist_raw = xci.cooling_degree_days(tas_ens_hist_raw).compute()\n",
    "cdds_mm_future_raw = xci.cooling_degree_days(tas_ens_future_raw).compute()\n",
    "\n",
    "# long-term means \n",
    "cdd_obs_ltm = cdd_obs.mean('time')\n",
    "cdds_mm_hist_raw_ltm = cdds_mm_hist_raw.mean('time')\n",
    "cdds_mm_future_raw_ltm = cdds_mm_future_raw.mean('time')\n",
    "\n",
    "# climate change delta\n",
    "cdds_mm_delta_raw = cdds_mm_future_raw_ltm - cdds_mm_hist_raw_ltm \n",
    "\n",
    "# multi-model mean change\n",
    "cdds_mm_delta_raw_ensmean = cdds_mm_delta_raw.mean('model')\n",
    "\n",
    "# represent model spread for each scenario by taking stdev across models for the change in long-term means\n",
    "cdds_mm_delta_raw_model_spread = cdds_mm_delta_raw.std('model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3007d50",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# plot changes to CDDs from the raw model output\n",
    "fig, ax = plt.subplots( figsize = (6, 4))\n",
    "ax.set_title(\"Edmonton Cooling Degree Days\\nChange in Long Term Mean (Raw Model Output)\")\n",
    "\n",
    "# plot long-term means\n",
    "bars = ax.bar(scenarios[1:], cdds_mm_delta_raw_ensmean.values, \n",
    "             yerr = cdds_mm_delta_raw_model_spread.values, \n",
    "             capsize = 5, error_kw = {'ecolor': 'k', 'elinewidth': 2})\n",
    "\n",
    "ax.set_ylabel('CDD')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae3f01",
   "metadata": {},
   "source": [
    "In this plot the blue bars show the multi-model mean change in the average annual CDDs, while the black errorbars represent the standard deviation across models for the given scenario. Clearly scenario uncertainty plays a large role in the overall range of projections, with the low end of the SSP5-8.5 projections not overlapping at all with the high end of the SSP2-1.6 projections. The errorbar doesn't overlap with zero for any scenario, so all scenarios project a statistically significant increase to CDDs.\n",
    "\n",
    "Interestingly, the model spread is larger for the higher forcing scenarios - in other words the model uncertainty grows with the strength of the forcing. Is that also true for the *percent uncertainty*? Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34804f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdds_mm_delta_raw_model_spread_pct = (cdds_mm_delta_raw_model_spread / cdds_mm_delta_raw_ensmean) * 100\n",
    "cdds_mm_delta_raw_model_spread_pct = cdds_mm_delta_raw_model_spread_pct.rename('pct_model_spread_raw')\n",
    "\n",
    "# mean across scenarios\n",
    "cdds_mm_delta_raw_model_spread_pct_scenmean = cdds_mm_delta_raw_model_spread_pct.mean('scenario')\n",
    "cdds_mm_delta_raw_model_spread_pct_scenmean['scenario'] = 'mean'\n",
    "\n",
    "model_spread_pct = xr.concat([cdds_mm_delta_raw_model_spread_pct, cdds_mm_delta_raw_model_spread_pct_scenmean],\n",
    "                             dim = 'scenario')\n",
    "\n",
    "# this will print out the results in a nice table, with the percentages rounded to 2 decimal places\n",
    "np.around(model_spread_pct, 2).to_dataframe().drop(['lon', 'lat'], axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4922fcc",
   "metadata": {},
   "source": [
    "The above table shows that the percent uncertainty isn't really larger for the higher forcing scenarios. In fact, the scenario with the largest relative model spread is the lowest forcing scenario. Proportionally, model uncertainty is just over 50% averaged across scenarios. \n",
    "\n",
    "We can also quantify the relative contribution of scenario uncertainty, by comparing the spread across the different SSPs to the change averaged across scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4bed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdds_raw_delta_scen_spread = cdds_mm_delta_raw_ensmean.std('scenario')\n",
    "cdds_delta_raw_scen_spread_pct = (cdds_raw_delta_scen_spread / cdds_mm_delta_raw_ensmean.mean('scenario')) * 100\n",
    "\n",
    "print(f\"Scenario Uncercainty for Raw Model Output: {np.around(cdds_delta_raw_scen_spread_pct.values, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50b78ea",
   "metadata": {},
   "source": [
    "In the raw model output, it appears that the relative contribution of scenario uncertainty is slightly larger than the typical magnitude of model uncertainty. This isn't so surprising, because the bar chart shows a very large disparity between the high and low forcing scenarios.\n",
    "\n",
    "We'll re-do these calculations for the bias-adjusted CDD projections to get a final assessment of the magnitude of model uncertainty and scenario uncerainty on the projections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad132a",
   "metadata": {},
   "source": [
    "## 6.3.3 Applying Bias-Correction\n",
    "\n",
    "Now it's time to apply the QDM bias correction to the model `tas` data. Like for the multi-model example, the `xclim` bias-adjustment routines apply separate adjustments over each dimension, so the function call is exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0bf39",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# fix the time axis chunking so xclim won't complain\n",
    "tas_ens_hist_raw = tas_ens_hist_raw.chunk({'time': -1})\n",
    "tas_ens_future_raw = tas_ens_future_raw.chunk({'time': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a62410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and then apply the QDM bias correction\n",
    "QDM_trained_mscen = sdba.adjustment.QuantileDeltaMapping.train(tas_obs_noleap,     \n",
    "                                                               tas_ens_hist_raw, \n",
    "                                                               nquantiles = 50, \n",
    "                                                               kind = \"+\",\n",
    "                                                               group = 'time.month' \n",
    "                                                              )\n",
    "\n",
    "\n",
    "tas_ens_hist_qdm = QDM_trained_mscen.adjust(tas_ens_hist_raw,                              \n",
    "                                            interp = 'linear')\n",
    "\n",
    "tas_ens_future_qdm = QDM_trained_mscen.adjust(tas_ens_future_raw,                                 \n",
    "                                              interp = 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3362f93",
   "metadata": {},
   "source": [
    "## 6.3.4 Validating Multi-Scenario Downscaled Data\n",
    "\n",
    "Like the previous examples, we'll start with plotting the daily climatologies. We'll again plot the multi-model mean for each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39043ab1",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# calculate daily climatologies for the multi-model ensemble\n",
    "tas_clim_hist_qdm = tas_ens_hist_qdm.groupby('time.dayofyear').mean('time').compute()\n",
    "tas_clim_future_qdm = tas_ens_future_qdm.groupby('time.dayofyear').mean('time').compute()\n",
    "\n",
    "# multi-model mean\n",
    "tas_clim_hist_qdm_ensmean = tas_clim_hist_qdm.mean('model')\n",
    "tas_clim_future_qdm_ensmean = tas_clim_future_qdm.mean('model')\n",
    "\n",
    "# add historical and future scenarios to the same DataArray so the xarray plotting routines show it on the legend\n",
    "tas_clim_hist_qdm_ensmean = tas_clim_hist_qdm_ensmean.assign_coords(scenario = 'historical')\n",
    "tas_clim_model_qdm = xr.concat([tas_clim_hist_qdm_ensmean, tas_clim_future_qdm_ensmean],\n",
    "                                dim = 'scenario')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f753e56e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# plot the daily climatologies for all scenarios, QDM\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "# daily climatologies as 1D curves\n",
    "# obs\n",
    "tas_dailyclim_obs.plot.line(ax = ax, label = \"Station Obs\", color = 'k', linewidth = 3)\n",
    "\n",
    "# models\n",
    "tas_clim_model_qdm.plot.line(ax = ax, hue = 'scenario')\n",
    "\n",
    "\n",
    "# 1 sigma shading\n",
    "# obs\n",
    "ax.fill_between(tas_dailyclim_obs.dayofyear,\n",
    "                tas_dailyclim_obs - tas_dailyclim_std_obs, \n",
    "                tas_dailyclim_obs + tas_dailyclim_std_obs,\n",
    "                alpha = 0.2, color = 'k')\n",
    "\n",
    "ax.set_title(\"Edmonton Daily Mean Temperature Daily Climatology\\nQDM Multi-Model Mean\")\n",
    "ax.set_ylabel('tas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d164481",
   "metadata": {},
   "source": [
    "The character of the changes to the daily climatology of `tas` isn't really changed by the bias correction, but the historical multi-model mean (blue) now matches the observed climatology (black) more closely. Next let's test the statistical significance of mean change to `tas` for each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe02b8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# calculate effective sample size for historical and future periods\n",
    "def effective_sample_size(data):\n",
    "    ntime = len(data.time)\n",
    "    # times not including the final timestep\n",
    "    times = data.time.isel(time = slice(0, ntime - 1))\n",
    "    # data not including the first timestep\n",
    "    data_lag = data.isel(time = slice(1, ntime))\n",
    "    # match up time values, otherwise the xr.corr function won't return the correct output\n",
    "    data_lag = data_lag.assign_coords(time = times)\n",
    "    \n",
    "    # calculate correlation\n",
    "    autocor = xr.corr(data.sel(time = times),\n",
    "                      data_lag,\n",
    "                      dim = 'time')\n",
    "    \n",
    "    neff = ntime * (1 - autocor) / (1 + autocor)\n",
    "    \n",
    "    return neff\n",
    "\n",
    "neff_hist_qdm = effective_sample_size(tas_ens_hist_qdm).mean('model')\n",
    "neff_future_qdm = effective_sample_size(tas_ens_future_qdm).mean('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ed055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and stdev for downscaled historical and future tas\n",
    "tas_ens_hist_qdm_mean = tas_ens_hist_qdm.mean(('model', 'time'))\n",
    "tas_ens_hist_qdm_stdev = np.sqrt(tas_ens_hist_qdm.var('time').mean('model'))\n",
    "\n",
    "tas_ens_future_qdm_mean = tas_ens_future_qdm.mean(('model', 'time'))\n",
    "tas_ens_future_qdm_stdev = np.sqrt(tas_ens_future_qdm.var('time').mean('model'))\n",
    "\n",
    "# perform two_sample t-test to see if future temperatures are higher than past\n",
    "pvals = []\n",
    "for scenario in tas_ens_future_qdm_mean.scenario.values:\n",
    "    tstat, pval = stats.ttest_ind_from_stats(tas_ens_future_qdm_mean.sel(scenario = scenario), \n",
    "                                            tas_ens_future_qdm_stdev.sel(scenario = scenario), \n",
    "                                            neff_future_qdm.sel(scenario = scenario), \n",
    "                                            tas_ens_hist_qdm_mean,\n",
    "                                            tas_ens_hist_qdm_stdev,\n",
    "                                            neff_hist_qdm, \n",
    "                                            equal_var = False,\n",
    "                                            alternative = 'greater') \n",
    "    pvals.append(pval.values)\n",
    "\n",
    "pd.DataFrame.from_dict({'scenario': tas_ens_future_qdm_mean.scenario.values, 'p-value': np.around(pvals, 4)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da380a38",
   "metadata": {},
   "source": [
    "The increase in mean `tas` is significant for all scenarios. \n",
    "\n",
    "\n",
    "## 6.2.5 Downscaled Indicator for Multiple Scenarios\n",
    "Next let's examine the changes to CDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50f2b1",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "tas_ens_hist_qdm.attrs['units'] = 'degC'\n",
    "tas_ens_future_qdm.attrs['units'] = 'degC'\n",
    "\n",
    "cdds_hist_qdm = xci.cooling_degree_days(tas_ens_hist_qdm).compute()\n",
    "cdds_future_qdm = xci.cooling_degree_days(tas_ens_future_qdm).compute()\n",
    "\n",
    "# long-term means\n",
    "cdds_hist_qdm_ltm = cdds_hist_qdm.mean('time')\n",
    "cdds_future_qdm_ltm = cdds_future_qdm.mean('time')\n",
    "\n",
    "# climate change deltas\n",
    "cdds_qdm_delta = cdds_future_qdm_ltm - cdds_hist_qdm_ltm\n",
    "\n",
    "# multi-model mean delta\n",
    "cdds_qdm_delta_ensmean = cdds_qdm_delta.mean('model')\n",
    "\n",
    "# stdev across models for delta\n",
    "cdds_qdm_delta_stdev = cdds_qdm_delta.std('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a4619",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# plot changes to CDDs from QDM data AND raw model data, next to each other\n",
    "fig, ax = plt.subplots(figsize = (6, 4), sharex = True, sharey = True)\n",
    "fig.suptitle(\"Edmonton Cooling Degree Days - Change in Long Term Mean\")\n",
    "\n",
    "# dummy x-axis values, which will be replaced by the scenario names later on.\n",
    "# we need to use these in order to have the bars for the two scenarios side-by-side\n",
    "# on the same axis\n",
    "xx = np.arange(len(scenarios[1:]))\n",
    "width = 0.4\n",
    "\n",
    "# raw model changes\n",
    "bars_raw = ax.bar(xx, cdds_mm_delta_raw_ensmean.values, width, \n",
    "                  yerr = cdds_mm_delta_raw_model_spread.values, \n",
    "                   # add hatching to make the two bars differ visually in another way than just the colour.\n",
    "                   # this helps make the plots more colorblind-friendly\n",
    "                  label = \"Raw Model Output\", hatch = 'xx',\n",
    "                  capsize = 5, error_kw = {'ecolor': 'k', 'elinewidth': 2})\n",
    "\n",
    "# QDM changes, plotted immediately to the right of the raw changes for the same scenario\n",
    "bars_qdm = ax.bar(xx + width, cdds_qdm_delta_ensmean.values, width, \n",
    "                  yerr = cdds_qdm_delta_stdev.values, \n",
    "                  label = \"QDM\", hatch = '..',\n",
    "                  capsize = 5, error_kw = {'ecolor': 'k', 'elinewidth': 2})\n",
    "\n",
    "\n",
    "# set up the x-tick labels (scenario names) to be in the middle of the two bars\n",
    "ax.set_xticks(xx + width/2)\n",
    "ax.set_xticklabels(scenarios[1:])\n",
    "\n",
    "# axis labels and legend \n",
    "ax.set_ylabel('CDD')\n",
    "ax.set_xlabel('Scenario')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78546fc9",
   "metadata": {},
   "source": [
    "The effect of the bias correction was to increase the magnitude of the CDD projections for each scenario, though the size of the model spread and scenario spread appears mostly unchanged. Like we did for the raw model projections, we can quantify the relative contributions of model uncertainty and scenario uncertainty by comparing the spread across models/scenarios to the mean change across models/scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f02d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdds_delta_qdm_model_spread_pct = (cdds_qdm_delta_stdev / cdds_qdm_delta_ensmean) * 100\n",
    "cdds_delta_qdm_model_spread_pct = cdds_delta_qdm_model_spread_pct.rename('pct_model_spread_qdm')\n",
    "\n",
    "cdds_delta_qdm_model_spread_pct_scenmean = cdds_delta_qdm_model_spread_pct.mean('scenario')\n",
    "cdds_delta_qdm_model_spread_pct_scenmean['scenario'] = 'mean'\n",
    "\n",
    "model_spread_pct_qdm = xr.concat([cdds_delta_qdm_model_spread_pct, cdds_delta_qdm_model_spread_pct_scenmean],\n",
    "                                  dim = 'scenario')\n",
    "np.around(model_spread_pct_qdm, 2).to_dataframe().drop(['lon', 'lat'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa45fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdds_qdm_delta_scen_spread = cdds_qdm_delta_ensmean.std('scenario')\n",
    "cdds_delta_qdm_scen_spread_pct = (cdds_qdm_delta_scen_spread / cdds_qdm_delta_ensmean.mean('scenario')) * 100\n",
    "\n",
    "print(f\"Scenario Uncercainty After Bias-Adjustment: {np.around(cdds_delta_qdm_scen_spread_pct.values, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87e45a",
   "metadata": {},
   "source": [
    "After applying the bias correction, the spread in projections across both models and scenarios is reduced, but scenario uncertainty still dominates model structural uncertainty, and now to a much greater extent. Therefore we'll use the spread across scenarios to generate our final range of changes to CDDs in Edmonton, from the historucal baseline period to the end of the 21st century. Again we'll use the 10th-90th percentile range to produce the overall uncerainty range, just this time across scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc83fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdd_change_10p = cdds_qdm_delta_ensmean.quantile(0.1, dim = 'scenario')\n",
    "cdd_change_90p = cdds_qdm_delta_ensmean.quantile(0.9, dim = 'scenario')\n",
    "\n",
    "# multi-model mean historical CDDs\n",
    "cdds_hist_qdm_ltm_mm_mean = cdds_hist_qdm_ltm.mean('model')\n",
    "\n",
    "results_str = \"Likely Range of Mean CDD Change from 1980-2010 to 2070-2100: \"\n",
    "results_str += f\"{int(np.around(100 * cdd_change_10p.values/cdds_hist_qdm_ltm_mm_mean.values, 0))}% to \"\n",
    "results_str +=f\"{int(np.around(100 * cdd_change_90p.values/cdds_hist_qdm_ltm_mm_mean.values, 0))}%\"\n",
    "\n",
    "print(results_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9c912d",
   "metadata": {},
   "source": [
    "This size of this projection range is similar to the range across models in Section 6.2, but because we're considering some weaker forcing scenarios, the magnitude of the changes is smaller than for the SSP3-7.0 scenario alone. \n",
    "\n",
    "Including multiple models and scenarios into this analysis has allowed us to characterize uncertainty in the future projections - something we couldn't do in the previous examples where either only one model or one scenario were investigated. \n",
    "\n",
    "So far none of the examples have assessed the contribution of internal climate variability to the uncertainty in future projections. For an end-of-century projection period and a variable like daily mean temperature which has a robust climate change signal, it's unlikely that internal variability would be comparable to model or scenario uncertainty. However, internal variability can be very important for near-term projections, when the magnitude of the climate change signal may not yet exceed the range of historical variability. In the next section, we'll attempt to prove this by using multiple realizations for a single model."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}