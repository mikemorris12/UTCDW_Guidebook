# 5.4 Producing Downscaled Projections

The next stage in the workflow is to use what you learned by exploring the observational and raw model data to select and apply your method for bias correcting and/or downscaling the model output to product calibrated projections of your required climate variables. In order to keep your workflow manageable, it is smart to work through the whole procedure using data from just one ensemble member from model, and once you've got things working, expand to additional ensemble members and models.

## 5.4.1 Choosing a Bias Correction/Downscaling Method

The best bias correction and/or downscaling methods to use depend on both your spatial sampling requirements, and the results of your investigation of the model biases. If you require gridded data with high spatial resolution, using a true "downscaling" method, such as a constructed analogue based method, is necessary. However, if all you need is point data at one or more distinct spatial locations, then a 1D bias correction method is sufficient, and there isn't a need to produce output on a fine spatial grid. We reccommend Quantile Delta Mapping (QDM) because of its ability to adjust biases in all quantiles of the distribution of a quantity, while also preserving projected changes in extreme values ([Cannon et al, 2015](https://doi.org/10.1175/JCLI-D-14-00754.1)).

If you wish to use an analogue method such as DBCCA or BCCAQ for spatial downscaling, the nature of the model biases are important to consider. Good agreement between the raw model output and observations means that only minor bias adjustment is required, and it is likely that there are observed analogues that share similar qualities to the model days being downscaled. If this is the case, then proceed with your preferred analogue downscaling method. Here we reccommend DBCCA, because of its ease of implementation relative to BCCAQ, while retaining good performance (as per [Murdock et al. (2014)](https://www.pacificclimate.org/sites/default/files/publications/PCIC_EC_downscaling_report_2014.pdf) and  [Werner and Cannon (2016)](https://doi.org/10.5194/hess-20-1483-2016)). Code for the DBCCA method is included in Chapter 6, along with examples implementing it.

If the raw model data looks very different from the observations, in terms of probability distributions, spatial patterns, or historical trends, then analogue-based downscaling methods may not be appropriate. The lowest 30 RMSE patterns from the observations might not be good matches to the model pattern, so the downscaled data would not be reflective of the underlying model projections, which aren't even trustworthy in the first place. Clearly things are much more difficult in this case, and the multivariate downscaling methods that might be more acceptable are outside of the scope of this guide. Alternatively, high-resolution climate simulations such as RCM data from [CORDEX](https://cordex.org/) or global data from [HighResMIP](https://highresmip.org/) may provide better raw model output to use as the starting point for statistical downscaling, though you should still perform the same validation you would for coarse model data. If high resolution model data still shows very poor agreement with observations for one or more of your required variables, then you'll need to look outside this guide for additional guidance regarding best practices for downscaling these climate variables.

## 5.4.2 Applying Bias Correction and Downscaling

Having chosen a downscaling or bias correction method, you're almost ready to apply it using your selected datasets. The last decision to be made is the way in which you'll apply the quantile-mapping (QM) based bias correction, be it alone or as a part of a spatial downscaling method like DBCCA. Recall that in the previous section of this chapter, we saw that the CanESM5 monthly mean temperatures in Toronto are biased high in the warm-season months and low in the cold-season months. Because the bias varies seasonally, it makes sense to separate (or "group") the data by the month of the year, and apply a different bias correction to each group. Calculating one set of adjustment factors for the entire dataset would leave remaining bias in the seasonal cycle of temperature. Understading the properties of the model bias will help you decide if a grouped bias correction is appropriate or not. In the `xclim.sdba` module that implements the different QM-based bias adjustment methods, you can specify the time unit by which you'd like to group the data using the `group` keyword argument to the `train` method of the adjustment method object (i.e. `xlim.sdba.adjustment.QuantileDeltaMapping.train(obs_data, hist_model_data, group = 'time.month')` for monthly groups). To avoid abrupt changes from one month to the next, you can specify an interpolation method that will be used to smooth the transitions using the `interp` keyword argument to the `adjust` method, i.e. `xlim.sdba.adjustment.QuantileDeltaMapping.adjust(future_model_data, interp = 'linear')` (or alternatively `interp = 'cubic'`). See documentation and examples of the `xclim.sdba` bias adjustment methods [here](https://xclim.readthedocs.io/en/stable/notebooks/sdba.html).

The type of climate change "delta" must also be specified when using either Quantile Delta Mapping or Detrended Quantile Mapping. The appropriate choice depends on the variable being downscaled. For interval such as temperature, an additive delta is to be used, and for ratio variables such as precipitation, a multiplicative delta is most appropriate. An interval variable is one where differences are meaningful, and a ratio variable is an interval variable with a defined absolute zero ([Finkelstein and Leaning, 1984](https://doi.org/10.1016/0263-2241(84)90020-4), [Cannon et al., 2015](https://doi.org/10.1175/JCLI-D-14-00754.1)). Temperature is consdiered an interval variable because it is typically measured in Celsius, where the zero is not an absolute zero. The type of delta is specified using the `kind` keyword argument in the `train` method of `xclim` adjustment objects (i.e. `xlim.sdba.adjustment.QuantileDeltaMapping.train(..., kind = "+")`).

Another decision to be made when applying QM bias corrections is the number of quantiles to estimate from the data, and how to interpolate adjustment factors between the computed quantiles. `xclim`'s implementation of quantile mapping does not assume any form of the underyling probability distribution of the data, it estimates a specified number of quantiles empirically, and interpolates between them when applying the bias adjustment. This is done for several reasons. First, it's difficult to specify a particular type of probability distribution that a climate variable should theoretically follow, so estimating quantiles empirically avoids this issue. Next, it's not reasonable statistically to sort each dataset and do a one-to-one mapping of the N'th highest model value to the N'th highest observed value, which is the same as choosing the same number of quantiles to estimate as there are data points. Since the data points are not all independent (due to autocorrelations, as discussed in Section 3.4) the true number of degrees of freedom is less than the number of data points, and using more parameters than data points is considered overfitting. Finally, estimating a moderate number of quantiles and interpolating, instead of direct one-to-one mapping of data values, is much faster computationally. The `xclim` default number of quantiles (specified by the keyword argument `nquantiles` in the `train` methods) is 20, but usually a larger number is appropriate. For many cases, only very minor differences are apparent when using `nquantiles` above 50 or so.

### 5.4.2.1 Data Cleaning and Additional Tips

In addition to the decisions made about the details of the bias correction methods, it's also important to remember that most climate data will not be ready to plug and play with the canned routines of `xclim` or other climate analysis packages, or there might be other minor things to consider to ensure a successful workflow. For the sake of brevity, these tips will be itemized below.

* Calendar consistency: Observational data is likely to include leap years, while most climate model data does not. You can exclude data on February 28th's using `xclim.core.convert_calendar` ([docs](https://xclim.readthedocs.io/en/stable/xclim.core.html#xclim.core.calendar.convert_calendar))

* Data units: Observational data and model output often are recorded using different units. For example, most observed precipitation data is in units of mm/day, but the standard CMIP6 unit for precipitation is kg m$^{-2}$ s$^{-1}$. To convert the model data to mm/day, you'll need to divide by the density of water (1000 kg m$^{-3}$), multiply by 1000 to convert m to mm, and then multiply by 86400 (to convert mm/s to mm/day). Same goes for temperature - model output is usually in Kelvin but observations are usually in degrees Celsius.

* Unit Attributes: `xclim` requires all variables (in `xarray.DataArray` format) to have a `units` attribute. Assign this using `da.attrs['units'] = unit_name` where `unit_name` is the appropriate unit for the quantity in `da` (i.e. `'K'`, `'degC'`, `'mm/day'`, etc.).

* Precipitation frequency: Climate models often have a "drizzle" bias, where the frequency of very small amounts of precipitation is too high, and the frequency of exactly zero precipitation ("dry days") is much too low. This can cause the low-quantile adjustment factors in quantile mapping to be poorly constrained. It's helpful to adjust the frequency of dry days to match that of observations using `xclim.sdba.processing.adapt_freq`. This function takes a keyword argument `thresh` that specifies the threshold below which days are considered to be dry days, if you don't wish to match the frequency of exact zeros. For example, you could call `adapt_freq(obs_pr, model_pr, thresh = 0.05 mm d-1)` if you wish to consider days with less than 0.05 mm of precipitation to be "dry days".

## 5.4.3 Validating the Downscaled Data

After you've got your downscaled climate variables, whether you did the downscaling yourself or you're using pre-produced downscaled projections, you must still do some more analysis before you can get to work on applying the data to your field-specific research question. 

### 5.4.3.1 Agreement with Observations

Section 4.4 demonstrated a number of metrics by which you may wish to validate the downscaled historical data against observations. As mentioned in that notebook, the best practice is to split the historical period into a training set, which you use to train the downscaling method (i.e. the dataset you use to select the analogue days, fit the quantile mapping bias correction, etc.) and a testing set (also called a validation set), which you use to independently assess the quality of agreement between the downscaled data and observations. As you saw in 4.4, this can be tricky for multiple reasons. First, if the observational record only goes back a few decades, each dataset will likely be smaller than the ideal minimum of 30 years. Second, if the model has a bias in the historical time trend, like CanESM5 does for near-surface temperatures (see examples in Section 4.4.4) then there may be poor agreement between the downscaled data and observations that could be mitigated by using all available observations to train the downscaling method. Since these issues can and will vary across different models, wait until you've been able to to validation on a number of models before deciding that the quality of the downscaled data is too poor to be acceptable and changing something about your workflow. If a model that produces poor results really stands out in the context of the multi-model spectrum, then it's okay to consider its projections less credible than the others, but it's dishonest to throw out its projections entirely.

Another important takeaway from Section 4.4 should be that you can do validation on both the downscaled climate variables *and* the climate indicator they're used to calculate. If your study involves using downscaled climate data as inputs for an impact model (like the VIC model in hydrology), then it might be feasible for you to also use observational data as inputs, and compare the results that used the downscaled historical data as inputs. When validating a climate indiactor or the outputs of your impact model, you'll need to leverage your expertise on the subject matter of your study and decide which properties are most important to get right. This could be having a low mean bias, good agreement in the probability distributions, good reproduction of spatial or temporal autocorrelation structures, or something that people outside of your field may not be familiar with. 

Similar to running your impact model/calculating your indicator using observational data, you should also consider doing this using the raw, non-downscaled model output. This may not be possible if your impact model explicitly requires high spatial resolution, since an important reason for downscaling is to increase the spatial resolution of the data, but if it is, you can compare the results for the downscaled data (both historical and future projections) to those for the raw model data, and check that the downscaled data indeed has reduced biases relative to observations. 

### 5.4.3.2 Significance of Climate Change Signal

Sections 3.4.5 and  4.5 demonstrated how one can test the statistical significance of projected changes to a climate variable or indicator under a future climate scenario. Because of natural fluctuations in the climate (i.e. internal variability), what might appear as a change due to climate change may still fall within the normal range of values under future climate. In order to make any claim that climate change has an effect on your quantities of interest, you need to test for statistical significance.

Like with validation, you can test the significance of changes to both the climate variables and your indicator(s)/impact model outputs, and for both raw and downscaled model data. Hopefully you did this for the raw model data already during the exploratory analysis phase, but if not, now is a good time to do so. It's important to have an idea of what the unadjusted model projections are, since the statistically downscaled data will (in cases covered in this guide) mainly reflect the raw model projections, with added spatial detail and reduced effects of model bias.

### 5.4.3.3 Credibility of Climate Change Signal

In addition to statistical significance, it's important to ensure that the downscaled future projections make sense physically. The first thing to do as a sanity check is to ensure that the downscaled projections are qualitatively similar to the raw model projections. At the very least, they should share the same sign (i.e. increases or decreases) as the raw model projections, for the univariate downscaling methods covered in this guide. If not, there is likely an error somewhere in your code. Next, you should be planning your study with some expectation of what the effect of climate change should be, based on a review of the relevant literature. For example, we expect to see projected increases to temperature, increases to precipitation in historically wet regions, and decreases to precipitation in historically arid regions. If your downscaled projections are in stark contrast to well-established effects of climate change, there is reason to doubt that your results are physically credible. It's not possible to list here all of the expected effects of climate change on every aspect of the climate system, so this is a matter that will require you to develop your own intuition and background knowledge by synthesizing the relevant previous work, including possibly the most rcent [IPCC Assessment Report](https://www.ipcc.ch/assessment-report/ar6/) (which does in fact discuss all known effects of climate change on all aspects of the climate system!).





