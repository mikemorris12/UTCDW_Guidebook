# 5.4 Producing Downscaled Data

The next stage in the workflow is to use what you learned by exploring the observational and raw model data to select and apply your method for bias correcting and/or downscaling the model output to product calibrated projections of your required climate variables.

## 5.4.1 Choosing a Bias Correction/Downscaling Method

The best bias correction and/or downscaling methods to use depend on both your spatial sampling requirements, and the results of your investigation of the model biases. If you require gridded data with high spatial resolution, using a true "downscaling" method, such as a constructed analogue based method, is necessary. However, if all you need is point data at one or more distinct spatial locations, then a 1D bias correction method is sufficient, and there isn't a need to produce output on a fine spatial grid. We reccommend Quantile Delta Mapping (QDM) because of its ability to adjust biases in all quantiles of the distribution of a quantity, while also preserving projected changes in extreme values ([Cannon et al, 2015](https://doi.org/10.1175/JCLI-D-14-00754.1)).

If you wish to use an analogue method such as DBCCA or BCCAQ for spatial downscaling, the nature of the model biases are important to consider. Good agreement between the raw model output and observations means that only minor bias adjustment is required, and it is likely that there are observed analogues that share similar qualities to the model days being downscaled. If this is the case, then proceed with your preferred analogue downscaling method. Here we reccommend DBCCA, because of its ease of implementation relative to BCCAQ, while retaining good performance (as per [Murdock et al. (2014)](https://www.pacificclimate.org/sites/default/files/publications/PCIC_EC_downscaling_report_2014.pdf) and  [Werner and Cannon (2016)](https://doi.org/10.5194/hess-20-1483-2016)). Code for the DBCCA method is included in Chapter 6, along with examples implementing it.

If the raw model data looks very different from the observations, in terms of probability distributions, spatial patterns, or historical trends, then analogue-based downscaling methods may not be appropriate. The lowest 30 RMSE patterns from the observations might not be good matches to the model pattern, so the downscaled data would not be reflective of the underlying model projections, which aren't even trustworthy in the first place. Clearly things are much more difficult in this case, and the multivariate downscaling methods that might be more acceptable are outside of the scope of this guide. Alternatively, high-resolution climate simulations such as RCM data from [CORDEX](https://cordex.org/) or global data from [HighResMIP](https://highresmip.org/) may provide better raw model output to use as the starting point for statistical downscaling, though you should still perform the same validation you would for coarse model data. If high resolution model data still shows very poor agreement with observations for one or more of your required variables, then you'll need to look outside this guide for additional guidance regarding best practices for downscaling these climate variables.

## 5.4.2 Applying Bias Correction and Downscaling

Having chosen a downscaling or bias correction method, you're almost ready to apply it using your selected datasets. The last decision to be made is the way in which you'll apply the quantile-mapping (QM) based bias correction, be it alone or as a part of a spatial downscaling method like DBCCA. Recall that in the previous section of this chapter, we saw that the CanESM5 monthly mean temperatures in Toronto are biased high in the warm-season months and low in the cold-season months. Because the bias varies seasonally, it makes sense to separate (or "group") the data by the month of the year, and apply a different bias correction to each group. Calculating one set of adjustment factors for the entire dataset would leave remaining bias in the seasonal cycle of temperature. Understading the properties of the model bias will help you decide if a grouped bias correction is appropriate or not. In the `xclim.sdba` module that implements the different QM-based bias adjustment methods, you can specify the time unit by which you'd like to group the data using the `group` keyword argument to the `train` method of the adjustment method object (i.e. `xlim.sdba.adjustment.QuantileDeltaMapping.train(obs_data, hist_model_data, group = 'time.month')` for monthly groups). To avoid abrupt changes from one month to the next, you can specify an interpolation method that will be used to smooth the transitions using the `interp` keyword argument to the `adjust` method, i.e. `xlim.sdba.adjustment.QuantileDeltaMapping.adjust(future_model_data, interp = 'linear')` (or alternatively `interp = 'cubic'`). See documentation and examples of the `xclim.sdba` bias adjustment methods [here](https://xclim.readthedocs.io/en/stable/notebooks/sdba.html).

The type of climate change "delta" must also be specified when using either Quantile Delta Mapping or Detrended Quantile Mapping. The appropriate choice depends on the variable being downscaled. For interval such as temperature, an additive delta is to be used, and for ratio variables such as precipitation, a multiplicative delta is most appropriate. An interval variable is one where differences are meaningful, and a ratio variable is an interval variable with a defined absolute zero ([Finkelstein and Leaning, 1984](https://doi.org/10.1016/0263-2241(84)90020-4), [Cannon et al., 2015](https://doi.org/10.1175/JCLI-D-14-00754.1)). Temperature is consdiered an interval variable because it is typically measured in Celsius, where the zero is not an absolute zero. The type of delta is specified using the `kind` keyword argument in the `train` method of `xclim` adjustment objects (i.e. `xlim.sdba.adjustment.QuantileDeltaMapping.train(..., kind = "+")`).

Another decision to be made when applying QM bias corrections is the number of quantiles to estimate from the data, and how to interpolate adjustment factors between the computed quantiles. `xclim`'s implementation of quantile mapping does not assume any form of the underyling probability distribution of the data, it estimates a specified number of quantiles empirically, and interpolates between them when applying the bias adjustment. This is done for several reasons. First, it's difficult to specify a particular type of probability distribution that a climate variable should theoretically follow, so estimating quantiles empirically avoids this issue. Next, it's not reasonable statistically to sort each dataset and do a one-to-one mapping of the N'th highest model value to the N'th highest observed value, which is the same as choosing the same number of quantiles to estimate as there are data points. Since the data points are not all independent (due to autocorrelations, as discussed in Section 3.4) the true number of degrees of freedom is less than the number of data points, and using more parameters than data points is considered overfitting. Finally, estimating a moderate number of quantiles and interpolating, instead of direct one-to-one mapping of data values, is much faster computationally. The `xclim` default number of quantiles (specified by the keyword argument `nquantiles` in the `train` methods) is 20, but usually a larger number is appropriate. For many cases, only very minor differences are apparent when using `nquantiles` above 50 or so.

### 5.4.2.1 Data Cleaning and Additional Tips

In addition to the decisions made about the details of the bias correction methods, it's also important to remember that most climate data will not be ready to plug and play with the canned routines of `xclim` or other climate analysis packages, or there might be other minor things to consider to ensure a successful workflow. For the sake of brevity, these tips will be itemized below.

* Calendar consistency: Observational data is likely to include leap years, while most climate model data does not. You can exclude data on February 28th's using `xclim.core.convert_calendar` ([docs](https://xclim.readthedocs.io/en/stable/xclim.core.html#xclim.core.calendar.convert_calendar))

* Data units: Observational data and model output often are recorded using different units. For example, most observed precipitation data is in units of mm/day, but the standard CMIP6 unit for precipitation is kg m$^{-2}$ s$^{-1}$. To convert the model data to mm/day, you'll need to divide by the density of water (1000 kg m$^{-3}$), multiply by 1000 to convert m to mm, and then multiply by 86400 (to convert mm/s to mm/day). Same goes for temperature - model output is usually in Kelvin but observations are usually in degrees Celsius.

* Unit Attributes: `xclim` requires all variables (in `xarray.DataArray` format) to have a `units` attribute. Assign this using `da.attrs['units'] = unit_name` where `unit_name` is the appropriate unit for the quantity in `da` (i.e. `'K'`, `'degC'`, `'mm/day'`, etc.).

* Precipitation frequency: Climate models often have a "drizzle" bias, where the frequency of very small amounts of precipitation is too high, and the frequency of exactly zero precipitation ("dry days") is much too low. This can cause the low-quantile adjustment factors in quantile mapping to be poorly constrained. It's helpful to adjust the frequency of dry days to match that of observations using `xclim.sdba.processing.adapt_freq`. This function takes a keyword argument `thresh` that specifies the threshold below which days are considered to be dry days, if you don't wish to match the frequency of exact zeros. For example, you could call `adapt_freq(obs_pr, model_pr, thresh = 0.05 mm d-1)` if you wish to consider days with less than 0.05 mm of precipitation to be "dry days".
